{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marketing Mix Modelling (MMM) - Multiplicative Model\n",
    "\n",
    "## Companion to Chapter 9b: Additive vs Multiplicative Models\n",
    "\n",
    "This notebook implements **multiplicative MMM specifications** as an alternative to the additive approach in the main notebook.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | Additive | Multiplicative |\n",
    "|--------|----------|----------------|\n",
    "| Formula | `Y = B + b1*X1 + b2*X2` | `Y = B * (1+L1) * (1+L2)` or `log(Y) = a + b1*log(X1)` |\n",
    "| Coefficients | Absolute dollar contribution | Elasticities (% change) |\n",
    "| Interactions | Must add explicitly | Natural/built-in |\n",
    "| Zero handling | Works fine | Requires transformation |\n",
    "| Decomposition | Simple sum | Shapley values needed |\n",
    "\n",
    "### When to Use Multiplicative\n",
    "\n",
    "- Channels genuinely interact (TV + Search synergy)\n",
    "- Business thinks in percentage lifts\n",
    "- You're comfortable with Shapley decomposition\n",
    "\n",
    "### This Notebook Covers\n",
    "\n",
    "1. **Log-Log Specification**: Elasticity-based model\n",
    "2. **Lift-Factor Specification**: Multiplicative lifts from baseline\n",
    "3. **Shapley Value Decomposition**: Fair attribution for multiplicative models\n",
    "4. **Comparison with Additive**: Same data, different approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bayesian modelling\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# For Shapley values\n",
    "from itertools import combinations\n",
    "from math import factorial\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part I: Data Preparation\n",
    "\n",
    "We use the same dataset and preprocessing as the additive notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_raw = pd.read_csv('conjura_mmm_data.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df_raw.shape}\")\n",
    "print(f\"Date range: {df_raw['DATE_DAY'].min()} to {df_raw['DATE_DAY'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find organisation with good data coverage\n",
    "spend_cols = [col for col in df_raw.columns if 'SPEND' in col]\n",
    "\n",
    "org_stats = df_raw.groupby('ORGANISATION_ID').agg({\n",
    "    'ALL_PURCHASES': 'sum',\n",
    "    'DATE_DAY': 'count',\n",
    "    **{col: lambda x: (x > 0).sum() for col in spend_cols}\n",
    "}).reset_index()\n",
    "\n",
    "org_stats.columns = ['ORGANISATION_ID', 'total_purchases', 'n_days'] + [f'{col}_days' for col in spend_cols]\n",
    "\n",
    "# Select organisation with highest purchases and good coverage\n",
    "selected_org = org_stats.nlargest(5, 'total_purchases').iloc[0]['ORGANISATION_ID']\n",
    "df_org = df_raw[df_raw['ORGANISATION_ID'] == selected_org].copy()\n",
    "\n",
    "print(f\"Selected organisation: {selected_org}\")\n",
    "print(f\"Records: {len(df_org)}\")\n",
    "print(f\"Total purchases: {df_org['ALL_PURCHASES'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to weekly data\n",
    "df_org['DATE_DAY'] = pd.to_datetime(df_org['DATE_DAY'])\n",
    "df_org['week'] = df_org['DATE_DAY'].dt.to_period('W').dt.start_time\n",
    "\n",
    "# Columns to aggregate\n",
    "numeric_cols = df_org.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cols_to_sum = [col for col in numeric_cols if col not in ['MMM_TIMESERIES_ID']]\n",
    "\n",
    "df_weekly = df_org.groupby('week')[cols_to_sum].sum().reset_index()\n",
    "df_weekly = df_weekly.rename(columns={'week': 'date'})\n",
    "\n",
    "print(f\"Weekly data shape: {df_weekly.shape}\")\n",
    "print(f\"Weeks: {len(df_weekly)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select media channels with meaningful spend\n",
    "spend_cols = [col for col in df_weekly.columns if 'SPEND' in col]\n",
    "channel_totals = df_weekly[spend_cols].sum()\n",
    "active_channels = channel_totals[channel_totals > channel_totals.max() * 0.05].index.tolist()\n",
    "\n",
    "print(\"Active media channels (>5% of max spend):\")\n",
    "for ch in active_channels:\n",
    "    print(f\"  {ch}: ${channel_totals[ch]:,.0f}\")\n",
    "\n",
    "media_channels = active_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working dataframe\n",
    "df = df_weekly[['date', 'ALL_PURCHASES'] + media_channels].copy()\n",
    "\n",
    "# Create trend variable\n",
    "df['trend'] = np.arange(len(df))\n",
    "\n",
    "# Create Fourier features for seasonality\n",
    "def create_fourier_features(n_periods, period=52, n_harmonics=3):\n",
    "    \"\"\"Create Fourier features for seasonality.\"\"\"\n",
    "    t = np.arange(n_periods)\n",
    "    features = []\n",
    "    names = []\n",
    "    for k in range(1, n_harmonics + 1):\n",
    "        features.append(np.sin(2 * np.pi * k * t / period))\n",
    "        features.append(np.cos(2 * np.pi * k * t / period))\n",
    "        names.extend([f'sin_{k}', f'cos_{k}'])\n",
    "    return np.column_stack(features), names\n",
    "\n",
    "X_fourier, fourier_names = create_fourier_features(len(df), period=52, n_harmonics=3)\n",
    "for i, name in enumerate(fourier_names):\n",
    "    df[name] = X_fourier[:, i]\n",
    "\n",
    "print(f\"Working dataframe shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part II: Transformation Functions\n",
    "\n",
    "Same adstock and saturation functions as the additive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_adstock(x, decay_rate):\n",
    "    \"\"\"\n",
    "    Apply geometric adstock transformation.\n",
    "    \n",
    "    Adstock(t) = Spend(t) + decay_rate * Adstock(t-1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Raw spend values\n",
    "    decay_rate : float\n",
    "        Decay parameter (0-1). Higher = longer carryover.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array : Adstocked values\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    adstocked = np.zeros_like(x)\n",
    "    adstocked[0] = x[0]\n",
    "    for t in range(1, len(x)):\n",
    "        adstocked[t] = x[t] + decay_rate * adstocked[t-1]\n",
    "    return adstocked\n",
    "\n",
    "\n",
    "def hill_function(x, K, S):\n",
    "    \"\"\"\n",
    "    Apply Hill saturation function.\n",
    "    \n",
    "    Response = x^S / (K^S + x^S)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        Adstocked spend\n",
    "    K : float\n",
    "        Half-saturation point (spend at 50% of max effect)\n",
    "    S : float\n",
    "        Shape parameter (steepness of curve)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array : Saturated values (0-1 range)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    return np.power(x, S) / (np.power(K, S) + np.power(x, S) + 1e-10)\n",
    "\n",
    "\n",
    "print(\"Transformation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part III: Prepare Data for Multiplicative Models\n",
    "\n",
    "Key difference: We need to handle zeros carefully for log transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "target_col = 'ALL_PURCHASES'\n",
    "y_raw = df[target_col].values.astype(np.float64)\n",
    "\n",
    "# Check for zeros in target (problematic for log)\n",
    "print(f\"Target zeros: {(y_raw == 0).sum()}\")\n",
    "print(f\"Target min: {y_raw.min():.2f}\")\n",
    "print(f\"Target mean: {y_raw.mean():.2f}\")\n",
    "\n",
    "# Scale target (for additive comparison later)\n",
    "y_mean = y_raw.mean()\n",
    "y_scaled = y_raw / y_mean\n",
    "\n",
    "# Log transform for multiplicative (use log1p for safety)\n",
    "y_log = np.log(y_raw + 1)  # log(y + 1) to handle any zeros\n",
    "\n",
    "print(f\"\\nLog-transformed target range: [{y_log.min():.2f}, {y_log.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare media data with adstock pre-applied\n",
    "n_obs = len(df)\n",
    "n_channels = len(media_channels)\n",
    "\n",
    "# Pre-compute adstocked spend for each channel\n",
    "# Using moderate decay rates as starting point\n",
    "default_decay = 0.5\n",
    "X_media_raw = np.zeros((n_obs, n_channels), dtype=np.float64)\n",
    "X_media_adstocked = np.zeros((n_obs, n_channels), dtype=np.float64)\n",
    "\n",
    "spend_means = {}\n",
    "\n",
    "for i, channel in enumerate(media_channels):\n",
    "    raw_spend = df[channel].values.astype(np.float64)\n",
    "    X_media_raw[:, i] = raw_spend\n",
    "    \n",
    "    # Apply adstock\n",
    "    adstocked = geometric_adstock(raw_spend, default_decay)\n",
    "    \n",
    "    # Scale by mean for numerical stability\n",
    "    mean_val = adstocked.mean() if adstocked.mean() > 0 else 1.0\n",
    "    spend_means[channel] = mean_val\n",
    "    X_media_adstocked[:, i] = adstocked / mean_val\n",
    "    \n",
    "    print(f\"{channel}: mean adstocked = {mean_val:,.0f}, zeros = {(raw_spend == 0).sum()}\")\n",
    "\n",
    "# For log-log model, we need log(1 + x) to handle zeros\n",
    "X_media_log = np.log1p(X_media_adstocked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare other variables\n",
    "trend = df['trend'].values.astype(np.float64)\n",
    "trend_scaled = trend / trend.max()  # Scale to [0, 1]\n",
    "\n",
    "X_fourier = df[fourier_names].values.astype(np.float64)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  y_log: {y_log.shape}\")\n",
    "print(f\"  X_media_log: {X_media_log.shape}\")\n",
    "print(f\"  X_fourier: {X_fourier.shape}\")\n",
    "print(f\"  trend_scaled: {trend_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part IV: Model 1 - Log-Log Multiplicative Model\n",
    "\n",
    "### The Specification\n",
    "\n",
    "```\n",
    "log(Y) = alpha + beta_trend * trend + beta_fourier * fourier + sum(beta_i * log(1 + X_i)) + epsilon\n",
    "```\n",
    "\n",
    "In this model:\n",
    "- **beta_i** represents the **elasticity** of channel i\n",
    "- Interpretation: A 1% increase in spend leads to a beta_i% increase in sales\n",
    "- Typical elasticities are small (0.01 - 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Log-Log Multiplicative Model\n",
    "\n",
    "with pm.Model() as log_log_model:\n",
    "    \n",
    "    # ============= BASELINE & CONTROLS =============\n",
    "    \n",
    "    # Intercept (in log space)\n",
    "    alpha = pm.Normal('alpha', mu=np.log(y_raw.mean()), sigma=1)\n",
    "    \n",
    "    # Trend coefficient (small effect expected)\n",
    "    beta_trend = pm.Normal('beta_trend', mu=0, sigma=0.5)\n",
    "    \n",
    "    # Seasonality coefficients (Fourier)\n",
    "    n_fourier = X_fourier.shape[1]\n",
    "    beta_fourier = pm.Normal('beta_fourier', mu=0, sigma=0.3, shape=n_fourier)\n",
    "    \n",
    "    # ============= MEDIA ELASTICITIES =============\n",
    "    # Elasticities are typically positive and small (0.01 to 0.3)\n",
    "    # Using HalfNormal to enforce positive effect\n",
    "    \n",
    "    beta_media = pm.HalfNormal('beta_media', sigma=0.15, shape=n_channels)\n",
    "    \n",
    "    # ============= NOISE =============\n",
    "    \n",
    "    sigma = pm.HalfNormal('sigma', sigma=0.5)\n",
    "    \n",
    "    # ============= BUILD LOG(MU) =============\n",
    "    \n",
    "    # Baseline (log scale)\n",
    "    log_mu = alpha\n",
    "    \n",
    "    # Trend component\n",
    "    log_mu = log_mu + beta_trend * trend_scaled\n",
    "    \n",
    "    # Seasonality component\n",
    "    log_mu = log_mu + pm.math.dot(X_fourier, beta_fourier)\n",
    "    \n",
    "    # Media effects (elasticities * log spend)\n",
    "    for i in range(n_channels):\n",
    "        log_mu = log_mu + beta_media[i] * X_media_log[:, i]\n",
    "    \n",
    "    # ============= LIKELIHOOD =============\n",
    "    # LogNormal distribution for the response\n",
    "    # This is equivalent to: log(Y) ~ Normal(log_mu, sigma)\n",
    "    \n",
    "    y_obs = pm.Normal('y_obs', mu=log_mu, sigma=sigma, observed=y_log)\n",
    "\n",
    "print(\"Log-Log Multiplicative Model specification complete!\")\n",
    "print(\"\\nModel interprets coefficients as ELASTICITIES:\")\n",
    "print(\"  beta = 0.10 means: 1% more spend -> 0.10% more sales\")\n",
    "print(log_log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Log-Log model\n",
    "with log_log_model:\n",
    "    trace_loglog = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=1000,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        target_accept=0.9,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        return_inferencedata=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence\n",
    "print(\"=\" * 60)\n",
    "print(\"LOG-LOG MODEL CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_loglog = az.summary(trace_loglog, var_names=['alpha', 'beta_trend', 'beta_media', 'sigma'])\n",
    "print(summary_loglog)\n",
    "\n",
    "# Check R-hat\n",
    "r_hat_ok = (summary_loglog['r_hat'] < 1.01).all()\n",
    "print(f\"\\nAll R-hat < 1.01: {r_hat_ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots for Log-Log model\n",
    "az.plot_trace(trace_loglog, var_names=['beta_media'], figsize=(12, 2*n_channels))\n",
    "plt.suptitle('Log-Log Model: Media Elasticities Trace Plots', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and interpret elasticities\n",
    "elasticities = trace_loglog.posterior['beta_media'].mean(dim=['chain', 'draw']).values\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MEDIA ELASTICITIES (Log-Log Model)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nInterpretation: 1% increase in spend -> X% increase in sales\\n\")\n",
    "\n",
    "for i, channel in enumerate(media_channels):\n",
    "    el = elasticities[i]\n",
    "    ci_low = np.percentile(trace_loglog.posterior['beta_media'].values[:, :, i], 2.5)\n",
    "    ci_high = np.percentile(trace_loglog.posterior['beta_media'].values[:, :, i], 97.5)\n",
    "    print(f\"{channel}:\")\n",
    "    print(f\"  Elasticity: {el:.4f} [{ci_low:.4f}, {ci_high:.4f}]\")\n",
    "    print(f\"  Meaning: 10% more spend -> {el*10:.2f}% more sales\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part V: Model 2 - Lift-Factor Multiplicative Model\n",
    "\n",
    "### The Specification\n",
    "\n",
    "```\n",
    "Y = Baseline * (1 + lift_TV) * (1 + lift_Digital) * ... * seasonality * exp(noise)\n",
    "```\n",
    "\n",
    "Where each lift is:\n",
    "```\n",
    "lift_i = gamma_i * saturate(adstock(spend_i))\n",
    "```\n",
    "\n",
    "This model:\n",
    "- Naturally captures channel interactions\n",
    "- Predictions are always positive\n",
    "- Baseline represents true \"no marketing\" sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lift-Factor Multiplicative Model\n",
    "\n",
    "with pm.Model() as lift_model:\n",
    "    \n",
    "    # ============= BASELINE =============\n",
    "    # Baseline sales with no marketing (log scale for positivity)\n",
    "    log_baseline = pm.Normal('log_baseline', mu=np.log(y_raw.mean() * 0.7), sigma=0.5)\n",
    "    baseline = pm.Deterministic('baseline', pm.math.exp(log_baseline))\n",
    "    \n",
    "    # ============= TREND MULTIPLIER =============\n",
    "    # Trend as a multiplicative factor\n",
    "    trend_coef = pm.Normal('trend_coef', mu=0, sigma=0.3)\n",
    "    trend_multiplier = 1 + trend_coef * trend_scaled\n",
    "    \n",
    "    # ============= SEASONALITY MULTIPLIER =============\n",
    "    n_fourier = X_fourier.shape[1]\n",
    "    beta_fourier = pm.Normal('beta_fourier', mu=0, sigma=0.2, shape=n_fourier)\n",
    "    seasonality_effect = pm.math.dot(X_fourier, beta_fourier)\n",
    "    seasonality_multiplier = 1 + seasonality_effect\n",
    "    \n",
    "    # ============= MEDIA LIFT FACTORS =============\n",
    "    # Gamma controls the maximum possible lift from each channel\n",
    "    # E.g., gamma=0.3 means channel can lift sales by up to 30%\n",
    "    gamma = pm.HalfNormal('gamma', sigma=0.3, shape=n_channels)\n",
    "    \n",
    "    # Saturation parameters (shared Hill function approach)\n",
    "    K = pm.LogNormal('K', mu=0, sigma=0.5, shape=n_channels)\n",
    "    S_raw = pm.Beta('S_raw', alpha=2, beta=2, shape=n_channels)\n",
    "    S = pm.Deterministic('S', 0.3 + 1.2 * S_raw)\n",
    "    \n",
    "    # ============= COMPUTE TOTAL LIFT =============\n",
    "    # Start with multiplier of 1\n",
    "    total_multiplier = trend_multiplier * seasonality_multiplier\n",
    "    \n",
    "    # Apply each channel's lift\n",
    "    for i in range(n_channels):\n",
    "        # Saturated media effect (0 to 1 range)\n",
    "        saturated = (X_media_adstocked[:, i] ** S[i]) / (K[i] ** S[i] + X_media_adstocked[:, i] ** S[i] + 1e-10)\n",
    "        \n",
    "        # Lift factor: 1 + gamma * saturated\n",
    "        channel_lift = 1 + gamma[i] * saturated\n",
    "        \n",
    "        # Multiply into total (this creates natural interactions!)\n",
    "        total_multiplier = total_multiplier * channel_lift\n",
    "    \n",
    "    # ============= EXPECTED VALUE =============\n",
    "    mu = baseline * total_multiplier\n",
    "    \n",
    "    # ============= NOISE =============\n",
    "    sigma = pm.HalfNormal('sigma', sigma=0.3)\n",
    "    \n",
    "    # ============= LIKELIHOOD =============\n",
    "    # LogNormal ensures positive predictions\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma * y_raw.mean(), observed=y_raw)\n",
    "\n",
    "print(\"Lift-Factor Multiplicative Model specification complete!\")\n",
    "print(\"\\nModel interprets gamma as MAXIMUM LIFT POTENTIAL:\")\n",
    "print(\"  gamma = 0.20 means: channel can lift sales by up to 20%\")\n",
    "print(lift_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Lift-Factor model\n",
    "with lift_model:\n",
    "    trace_lift = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=1500,\n",
    "        chains=4,\n",
    "        cores=4,\n",
    "        target_accept=0.95,  # Higher for multiplicative model stability\n",
    "        random_seed=RANDOM_SEED,\n",
    "        return_inferencedata=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence\n",
    "print(\"=\" * 60)\n",
    "print(\"LIFT-FACTOR MODEL CONVERGENCE DIAGNOSTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_lift = az.summary(trace_lift, var_names=['baseline', 'gamma', 'K', 'S', 'sigma'])\n",
    "print(summary_lift)\n",
    "\n",
    "# Check R-hat\n",
    "r_hat_ok = (summary_lift['r_hat'] < 1.05).all()\n",
    "print(f\"\\nAll R-hat < 1.05: {r_hat_ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots for Lift model\n",
    "az.plot_trace(trace_lift, var_names=['gamma', 'baseline'], figsize=(12, 2*(n_channels+1)))\n",
    "plt.suptitle('Lift-Factor Model: Gamma (Max Lift) Trace Plots', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and interpret lift factors\n",
    "gammas = trace_lift.posterior['gamma'].mean(dim=['chain', 'draw']).values\n",
    "baseline_est = trace_lift.posterior['baseline'].mean(dim=['chain', 'draw']).values\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LIFT FACTORS (Multiplicative Model)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBaseline (no marketing): {baseline_est:,.0f} sales/week\")\n",
    "print(f\"Average actual sales: {y_raw.mean():,.0f} sales/week\")\n",
    "print(f\"Implied total lift: {(y_raw.mean() / baseline_est - 1)*100:.1f}%\")\n",
    "print(\"\\nChannel Maximum Lift Potential:\\n\")\n",
    "\n",
    "for i, channel in enumerate(media_channels):\n",
    "    g = gammas[i]\n",
    "    ci_low = np.percentile(trace_lift.posterior['gamma'].values[:, :, i], 2.5)\n",
    "    ci_high = np.percentile(trace_lift.posterior['gamma'].values[:, :, i], 97.5)\n",
    "    print(f\"{channel}:\")\n",
    "    print(f\"  Max lift (gamma): {g:.3f} [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "    print(f\"  Meaning: At saturation, lifts sales by up to {g*100:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part VI: Shapley Value Decomposition\n",
    "\n",
    "In multiplicative models, simple subtraction doesn't work for attribution.\n",
    "Shapley values provide a **fair** decomposition:\n",
    "\n",
    "- Each channel gets credit based on its **marginal contribution** across all possible orderings\n",
    "- The sum of Shapley values equals the total effect\n",
    "- Satisfies fairness axioms (symmetry, efficiency, linearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shapley_values(baseline, channel_lifts):\n",
    "    \"\"\"\n",
    "    Compute Shapley values for multiplicative attribution.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    baseline : float\n",
    "        Baseline sales (no marketing)\n",
    "    channel_lifts : dict\n",
    "        Dictionary of {channel_name: lift_array} where lift_array\n",
    "        contains the multiplicative lift (1 + effect) for each time period\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Shapley value contribution for each channel\n",
    "    \"\"\"\n",
    "    channels = list(channel_lifts.keys())\n",
    "    n = len(channels)\n",
    "    n_obs = len(list(channel_lifts.values())[0])\n",
    "    \n",
    "    # Initialize Shapley values\n",
    "    shapley = {ch: np.zeros(n_obs) for ch in channels}\n",
    "    \n",
    "    # For each channel, compute its marginal contribution across all coalitions\n",
    "    for i, channel in enumerate(channels):\n",
    "        other_channels = [c for c in channels if c != channel]\n",
    "        \n",
    "        # Iterate over all possible subsets of other channels\n",
    "        for size in range(len(other_channels) + 1):\n",
    "            for subset in combinations(other_channels, size):\n",
    "                # Coalition without the channel\n",
    "                value_without = baseline.copy() if isinstance(baseline, np.ndarray) else np.full(n_obs, baseline)\n",
    "                for c in subset:\n",
    "                    value_without = value_without * channel_lifts[c]\n",
    "                \n",
    "                # Coalition with the channel\n",
    "                value_with = value_without * channel_lifts[channel]\n",
    "                \n",
    "                # Marginal contribution\n",
    "                marginal = value_with - value_without\n",
    "                \n",
    "                # Shapley weight\n",
    "                weight = (factorial(size) * factorial(n - size - 1)) / factorial(n)\n",
    "                \n",
    "                shapley[channel] += weight * marginal\n",
    "    \n",
    "    return shapley\n",
    "\n",
    "print(\"Shapley value function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Shapley values for the Lift-Factor model\n",
    "\n",
    "# Extract posterior means\n",
    "gamma_means = trace_lift.posterior['gamma'].mean(dim=['chain', 'draw']).values\n",
    "K_means = trace_lift.posterior['K'].mean(dim=['chain', 'draw']).values\n",
    "S_means = trace_lift.posterior['S'].mean(dim=['chain', 'draw']).values\n",
    "baseline_mean = trace_lift.posterior['baseline'].mean(dim=['chain', 'draw']).values\n",
    "\n",
    "# Compute lift factors for each channel\n",
    "channel_lifts = {}\n",
    "for i, channel in enumerate(media_channels):\n",
    "    saturated = hill_function(X_media_adstocked[:, i], K_means[i], S_means[i])\n",
    "    lift = 1 + gamma_means[i] * saturated\n",
    "    channel_lifts[channel] = lift\n",
    "\n",
    "# Compute Shapley values\n",
    "shapley_values = compute_shapley_values(baseline_mean, channel_lifts)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SHAPLEY VALUE DECOMPOSITION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTotal contribution by channel (summed over all weeks):\\n\")\n",
    "\n",
    "total_shapley = sum(sv.sum() for sv in shapley_values.values())\n",
    "for channel in media_channels:\n",
    "    sv_total = shapley_values[channel].sum()\n",
    "    sv_pct = (sv_total / total_shapley) * 100 if total_shapley > 0 else 0\n",
    "    print(f\"{channel}:\")\n",
    "    print(f\"  Total Shapley contribution: {sv_total:,.0f}\")\n",
    "    print(f\"  Share of media effect: {sv_pct:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Shapley decomposition over time\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Stack the Shapley contributions\n",
    "bottom = np.full(n_obs, baseline_mean)\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_channels))\n",
    "\n",
    "for i, channel in enumerate(media_channels):\n",
    "    ax.fill_between(df['date'], bottom, bottom + shapley_values[channel], \n",
    "                    alpha=0.7, label=channel, color=colors[i])\n",
    "    bottom = bottom + shapley_values[channel]\n",
    "\n",
    "# Add baseline\n",
    "ax.axhline(y=baseline_mean, color='black', linestyle='--', label='Baseline', alpha=0.5)\n",
    "\n",
    "# Add actual sales\n",
    "ax.plot(df['date'], y_raw, 'k-', linewidth=1.5, label='Actual Sales', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales')\n",
    "ax.set_title('Shapley Value Decomposition (Multiplicative Model)')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part VII: Model Comparison\n",
    "\n",
    "Compare the Log-Log and Lift-Factor models using:\n",
    "- Predictive accuracy (MAPE, R-squared)\n",
    "- Implied ROI/elasticities\n",
    "- Model fit (LOO-CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from Log-Log model\n",
    "with log_log_model:\n",
    "    ppc_loglog = pm.sample_posterior_predictive(trace_loglog, random_seed=RANDOM_SEED)\n",
    "\n",
    "# Predictions are in log space, need to transform back\n",
    "y_pred_loglog_log = ppc_loglog.posterior_predictive['y_obs'].mean(dim=['chain', 'draw']).values\n",
    "y_pred_loglog = np.exp(y_pred_loglog_log) - 1  # Inverse of log(y+1)\n",
    "\n",
    "# Metrics for Log-Log\n",
    "r2_loglog = r2_score(y_raw, y_pred_loglog)\n",
    "mape_loglog = mean_absolute_percentage_error(y_raw, y_pred_loglog) * 100\n",
    "\n",
    "print(\"Log-Log Model Performance:\")\n",
    "print(f\"  R-squared: {r2_loglog:.3f}\")\n",
    "print(f\"  MAPE: {mape_loglog:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from Lift-Factor model\n",
    "with lift_model:\n",
    "    ppc_lift = pm.sample_posterior_predictive(trace_lift, random_seed=RANDOM_SEED)\n",
    "\n",
    "y_pred_lift = ppc_lift.posterior_predictive['y_obs'].mean(dim=['chain', 'draw']).values\n",
    "\n",
    "# Metrics for Lift-Factor\n",
    "r2_lift = r2_score(y_raw, y_pred_lift)\n",
    "mape_lift = mean_absolute_percentage_error(y_raw, y_pred_lift) * 100\n",
    "\n",
    "print(\"Lift-Factor Model Performance:\")\n",
    "print(f\"  R-squared: {r2_lift:.3f}\")\n",
    "print(f\"  MAPE: {mape_lift:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series: Log-Log\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df['date'], y_raw, 'b-', label='Actual', alpha=0.7)\n",
    "ax.plot(df['date'], y_pred_loglog, 'r--', label='Log-Log Predicted', alpha=0.7)\n",
    "ax.set_title(f'Log-Log Model (R2={r2_loglog:.3f}, MAPE={mape_loglog:.1f}%)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales')\n",
    "\n",
    "# Time series: Lift-Factor\n",
    "ax = axes[0, 1]\n",
    "ax.plot(df['date'], y_raw, 'b-', label='Actual', alpha=0.7)\n",
    "ax.plot(df['date'], y_pred_lift, 'g--', label='Lift Predicted', alpha=0.7)\n",
    "ax.set_title(f'Lift-Factor Model (R2={r2_lift:.3f}, MAPE={mape_lift:.1f}%)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales')\n",
    "\n",
    "# Scatter: Log-Log\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(y_raw, y_pred_loglog, alpha=0.5)\n",
    "ax.plot([y_raw.min(), y_raw.max()], [y_raw.min(), y_raw.max()], 'r--')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title('Log-Log: Actual vs Predicted')\n",
    "\n",
    "# Scatter: Lift-Factor\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(y_raw, y_pred_lift, alpha=0.5, color='green')\n",
    "ax.plot([y_raw.min(), y_raw.max()], [y_raw.min(), y_raw.max()], 'r--')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title('Lift-Factor: Actual vs Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['R-squared', 'MAPE (%)', 'Coefficient Type', 'Handles Zeros', 'Natural Interactions'],\n",
    "    'Log-Log': [f'{r2_loglog:.3f}', f'{mape_loglog:.1f}', 'Elasticities', 'Yes (log1p)', 'No'],\n",
    "    'Lift-Factor': [f'{r2_lift:.3f}', f'{mape_lift:.1f}', 'Lift %', 'Yes', 'Yes']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part VIII: ROI Calculation for Multiplicative Models\n",
    "\n",
    "ROI calculation differs between model types:\n",
    "\n",
    "- **Log-Log**: ROI = (elasticity * average_sales) / average_spend\n",
    "- **Lift-Factor**: Use Shapley values for contribution, then ROI = contribution / spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI from Shapley values (Lift-Factor model)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ROI CALCULATION (Lift-Factor Model via Shapley)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "roi_shapley = {}\n",
    "for i, channel in enumerate(media_channels):\n",
    "    total_spend = df[channel].sum()\n",
    "    total_contribution = shapley_values[channel].sum()\n",
    "    \n",
    "    if total_spend > 0:\n",
    "        roi = total_contribution / total_spend\n",
    "    else:\n",
    "        roi = 0\n",
    "    \n",
    "    roi_shapley[channel] = roi\n",
    "    print(f\"\\n{channel}:\")\n",
    "    print(f\"  Total Spend: ${total_spend:,.0f}\")\n",
    "    print(f\"  Shapley Contribution: {total_contribution:,.0f} sales\")\n",
    "    print(f\"  ROI: {roi:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI from elasticities (Log-Log model)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ROI CALCULATION (Log-Log Model via Elasticities)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNote: Elasticity-based ROI is approximate\\n\")\n",
    "\n",
    "avg_sales = y_raw.mean()\n",
    "roi_elasticity = {}\n",
    "\n",
    "for i, channel in enumerate(media_channels):\n",
    "    elasticity = elasticities[i]\n",
    "    avg_spend = df[channel].mean()\n",
    "    \n",
    "    if avg_spend > 0:\n",
    "        # ROI approximation: (elasticity * avg_sales) / avg_spend\n",
    "        # This gives: d(Sales)/d(Spend) at the average\n",
    "        roi = (elasticity * avg_sales) / avg_spend\n",
    "    else:\n",
    "        roi = 0\n",
    "    \n",
    "    roi_elasticity[channel] = roi\n",
    "    print(f\"{channel}:\")\n",
    "    print(f\"  Elasticity: {elasticity:.4f}\")\n",
    "    print(f\"  Avg Spend: ${avg_spend:,.0f}\")\n",
    "    print(f\"  Approximate ROI: {roi:.2f}x\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual ROI comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(media_channels))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, [roi_elasticity.get(ch, 0) for ch in media_channels], \n",
    "               width, label='Log-Log (Elasticity)', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, [roi_shapley.get(ch, 0) for ch in media_channels], \n",
    "               width, label='Lift-Factor (Shapley)', color='seagreen')\n",
    "\n",
    "ax.set_xlabel('Channel')\n",
    "ax.set_ylabel('ROI')\n",
    "ax.set_title('ROI Comparison: Log-Log vs Lift-Factor Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([ch.replace('_SPEND', '') for ch in media_channels], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Break-even')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part IX: Budget Optimization (Multiplicative Models)\n",
    "\n",
    "Budget optimization for multiplicative models uses the same principle as additive:\n",
    "**Equalize marginal ROI across channels.**\n",
    "\n",
    "However, the calculation differs:\n",
    "\n",
    "- **Log-Log**: Marginal effect = elasticity * (Y / X) at current levels\n",
    "- **Lift-Factor**: Use Shapley-based marginal contributions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_marginal_roi_loglog(spend, elasticity, avg_sales, avg_spend):\n",
    "    \"\"\"\n",
    "    Calculate marginal ROI for log-log model.\n",
    "    \n",
    "    For log-log: d(Sales)/d(Spend) = elasticity * (Sales / Spend)\n",
    "    \n",
    "    At a given spend level, marginal ROI decreases as spend increases\n",
    "    (because Sales/Spend ratio changes).\n",
    "    \"\"\"\n",
    "    # Approximate sales at this spend level using elasticity\n",
    "    # log(Y) = log(Y_avg) + elasticity * log(Spend / Spend_avg)\n",
    "    sales_at_spend = avg_sales * (spend / avg_spend) ** elasticity\n",
    "    \n",
    "    # Marginal effect: d(Sales)/d(Spend) = elasticity * Sales / Spend\n",
    "    marginal_sales = elasticity * sales_at_spend / spend if spend > 0 else 0\n",
    "    \n",
    "    return marginal_sales\n",
    "\n",
    "\n",
    "def calculate_marginal_roi_lift(spend, gamma, K, S, baseline, avg_spend):\n",
    "    \"\"\"\n",
    "    Calculate marginal ROI for lift-factor model.\n",
    "    \n",
    "    Lift = 1 + gamma * Hill(spend)\n",
    "    Marginal = baseline * gamma * Hill'(spend)\n",
    "    \"\"\"\n",
    "    # Scale spend\n",
    "    x = spend / avg_spend if avg_spend > 0 else spend\n",
    "    \n",
    "    # Hill derivative: d/dx [x^S / (K^S + x^S)] = S * K^S * x^(S-1) / (K^S + x^S)^2\n",
    "    if x > 0:\n",
    "        numerator = S * (K ** S) * (x ** (S - 1))\n",
    "        denominator = (K ** S + x ** S) ** 2 + 1e-10\n",
    "        hill_deriv = numerator / denominator\n",
    "    else:\n",
    "        hill_deriv = 0\n",
    "    \n",
    "    # Marginal contribution (in sales units)\n",
    "    marginal = baseline * gamma * hill_deriv / avg_spend if avg_spend > 0 else 0\n",
    "    \n",
    "    return marginal\n",
    "\n",
    "print(\"Marginal ROI functions defined.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate current marginal ROI for each channel (Log-Log model)\n",
    "print(\"=\" * 60)\n",
    "print(\"CURRENT MARGINAL ROI (Log-Log Model)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nMarginal ROI = return on the NEXT dollar spent\\n\")\n",
    "\n",
    "avg_sales = y_raw.mean()\n",
    "marginal_rois_loglog = {}\n",
    "\n",
    "for i, channel in enumerate(media_channels):\n",
    "    avg_spend = df[channel].mean()\n",
    "    current_spend = avg_spend  # Evaluate at current spend level\n",
    "    \n",
    "    mroi = calculate_marginal_roi_loglog(\n",
    "        spend=current_spend,\n",
    "        elasticity=elasticities[i],\n",
    "        avg_sales=avg_sales,\n",
    "        avg_spend=avg_spend\n",
    "    )\n",
    "    \n",
    "    marginal_rois_loglog[channel] = mroi\n",
    "    print(f\"{channel.replace('_SPEND', '')}:\")\n",
    "    print(f\"  Current weekly spend: ${current_spend:,.0f}\")\n",
    "    print(f\"  Elasticity: {elasticities[i]:.4f}\")\n",
    "    print(f\"  Marginal ROI: {mroi:.4f}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def optimize_budget_loglog(total_budget, channels, elasticities_dict, \n",
    "                           avg_sales, avg_spends, min_pct=0.1, max_pct=0.8):\n",
    "    \"\"\"\n",
    "    Optimize budget allocation for log-log model.\n",
    "    \n",
    "    Maximizes total sales given budget constraint.\n",
    "    \"\"\"\n",
    "    n = len(channels)\n",
    "    \n",
    "    def neg_total_sales(allocation):\n",
    "        \"\"\"Negative total sales (for minimization).\"\"\"\n",
    "        total = 0\n",
    "        for i, ch in enumerate(channels):\n",
    "            spend = allocation[i]\n",
    "            el = elasticities_dict[ch]\n",
    "            avg_sp = avg_spends[ch]\n",
    "            # Sales contribution: avg_sales * (spend/avg_spend)^elasticity\n",
    "            if spend > 0 and avg_sp > 0:\n",
    "                contribution = avg_sales * (spend / avg_sp) ** el\n",
    "            else:\n",
    "                contribution = 0\n",
    "            total += contribution\n",
    "        return -total\n",
    "    \n",
    "    # Constraints: sum of allocation = total budget\n",
    "    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - total_budget}]\n",
    "    \n",
    "    # Bounds: min and max spend per channel\n",
    "    bounds = [(total_budget * min_pct / n, total_budget * max_pct) for _ in range(n)]\n",
    "    \n",
    "    # Initial guess: equal split\n",
    "    x0 = np.array([total_budget / n] * n)\n",
    "    \n",
    "    # Optimize\n",
    "    result = minimize(neg_total_sales, x0, method='SLSQP',\n",
    "                      bounds=bounds, constraints=constraints)\n",
    "    \n",
    "    if result.success:\n",
    "        return {\n",
    "            'success': True,\n",
    "            'allocation': {ch: result.x[i] for i, ch in enumerate(channels)},\n",
    "            'total_sales': -result.fun\n",
    "        }\n",
    "    else:\n",
    "        return {'success': False, 'message': result.message}\n",
    "\n",
    "print(\"Budget optimization function defined.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run budget optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"BUDGET OPTIMIZATION (Log-Log Model)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare inputs\n",
    "elasticities_dict = {ch: elasticities[i] for i, ch in enumerate(media_channels)}\n",
    "avg_spends = {ch: df[ch].mean() for ch in media_channels}\n",
    "current_budget = sum(avg_spends.values())\n",
    "\n",
    "print(f\"\\nCurrent total weekly budget: ${current_budget:,.0f}\")\n",
    "\n",
    "# Optimize\n",
    "opt_result = optimize_budget_loglog(\n",
    "    total_budget=current_budget,\n",
    "    channels=media_channels,\n",
    "    elasticities_dict=elasticities_dict,\n",
    "    avg_sales=avg_sales,\n",
    "    avg_spends=avg_spends,\n",
    "    min_pct=0.05,\n",
    "    max_pct=0.9\n",
    ")\n",
    "\n",
    "if opt_result['success']:\n",
    "    print(\"\\nOptimization successful!\")\n",
    "    print(\"\\nCurrent vs Optimal Allocation:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    comparison_data = []\n",
    "    for ch in media_channels:\n",
    "        current = avg_spends[ch]\n",
    "        optimal = opt_result['allocation'][ch]\n",
    "        change_pct = 100 * (optimal - current) / current if current > 0 else 0\n",
    "        comparison_data.append({\n",
    "            'Channel': ch.replace('_SPEND', ''),\n",
    "            'Current': current,\n",
    "            'Optimal': optimal,\n",
    "            'Change (%)': change_pct\n",
    "        })\n",
    "        print(f\"{ch.replace('_SPEND', '')}:\")\n",
    "        print(f\"  Current: ${current:,.0f}\")\n",
    "        print(f\"  Optimal: ${optimal:,.0f}\")\n",
    "        print(f\"  Change: {change_pct:+.1f}%\")\n",
    "        print()\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "else:\n",
    "    print(f\"\\nOptimization failed: {opt_result.get('message', 'Unknown error')}\")\n",
    "    comparison_df = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize current vs optimal allocation\n",
    "if opt_result['success']:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    channels_display = [c.replace('_SPEND', '') for c in media_channels]\n",
    "    current_values = [avg_spends[ch] for ch in media_channels]\n",
    "    optimal_values = [opt_result['allocation'][ch] for ch in media_channels]\n",
    "    \n",
    "    # Plot 1: Bar chart - Current vs Optimal\n",
    "    ax = axes[0]\n",
    "    x = np.arange(len(channels_display))\n",
    "    width = 0.35\n",
    "    bars1 = ax.bar(x - width/2, current_values, width, label='Current', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, optimal_values, width, label='Optimal', color='coral')\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Weekly Spend ($)')\n",
    "    ax.set_title('Current vs Optimal Budget Allocation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(channels_display, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Change percentage\n",
    "    ax = axes[1]\n",
    "    changes = comparison_df['Change (%)'].values\n",
    "    colors = ['green' if c > 0 else 'red' for c in changes]\n",
    "    ax.barh(channels_display, changes, color=colors, alpha=0.7)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_xlabel('Change (%)')\n",
    "    ax.set_title('Recommended Budget Changes')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 3: Pie charts\n",
    "    ax = axes[2]\n",
    "    # Create side-by-side pie charts using subplots\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create inset axes for pie charts\n",
    "    ax1 = fig.add_axes([0.68, 0.15, 0.15, 0.7])  # Current\n",
    "    ax2 = fig.add_axes([0.84, 0.15, 0.15, 0.7])  # Optimal\n",
    "    \n",
    "    ax1.pie(current_values, labels=channels_display, autopct='%1.0f%%', \n",
    "            colors=plt.cm.Set2(np.linspace(0, 1, len(channels_display))))\n",
    "    ax1.set_title('Current', fontsize=10)\n",
    "    \n",
    "    ax2.pie(optimal_values, labels=channels_display, autopct='%1.0f%%',\n",
    "            colors=plt.cm.Set2(np.linspace(0, 1, len(channels_display))))\n",
    "    ax2.set_title('Optimal', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot visualize - optimization did not succeed.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate expected improvement\n",
    "if opt_result['success']:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXPECTED IMPROVEMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate current expected sales\n",
    "    current_sales = 0\n",
    "    for i, ch in enumerate(media_channels):\n",
    "        spend = avg_spends[ch]\n",
    "        el = elasticities[i]\n",
    "        contribution = avg_sales * (spend / avg_spends[ch]) ** el if spend > 0 else 0\n",
    "        current_sales += contribution\n",
    "    \n",
    "    # Optimal sales from optimization\n",
    "    optimal_sales = opt_result['total_sales']\n",
    "    \n",
    "    improvement = (optimal_sales - current_sales) / current_sales * 100 if current_sales > 0 else 0\n",
    "    \n",
    "    print(f\"\\nCurrent expected media-driven sales: {current_sales:,.0f}\")\n",
    "    print(f\"Optimal expected media-driven sales: {optimal_sales:,.0f}\")\n",
    "    print(f\"Expected improvement: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Marginal ROI at optimal allocation\n",
    "    print(\"\\nMarginal ROI at Optimal Allocation:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, ch in enumerate(media_channels):\n",
    "        opt_spend = opt_result['allocation'][ch]\n",
    "        mroi = calculate_marginal_roi_loglog(\n",
    "            spend=opt_spend,\n",
    "            elasticity=elasticities[i],\n",
    "            avg_sales=avg_sales,\n",
    "            avg_spend=avg_spends[ch]\n",
    "        )\n",
    "        print(f\"  {ch.replace('_SPEND', '')}: {mroi:.4f}\")\n",
    "    \n",
    "    print(\"\\n(At optimum, marginal ROIs should be approximately equal)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part X: Summary and Recommendations\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Log-Log Model**:\n",
    "   - Coefficients are elasticities (% change in sales per % change in spend)\n",
    "   - Simpler to implement and interpret\n",
    "   - Good when interactions are not the primary concern\n",
    "\n",
    "2. **Lift-Factor Model**:\n",
    "   - Naturally captures channel interactions through multiplication\n",
    "   - Requires Shapley values for fair decomposition\n",
    "   - Better when synergies between channels are important\n",
    "\n",
    "3. **Shapley Values**:\n",
    "   - Essential for fair attribution in multiplicative models\n",
    "   - Computationally expensive (2^n coalitions)\n",
    "   - Satisfies key fairness axioms\n",
    "\n",
    "### When to Choose Each Model\n",
    "\n",
    "| Scenario | Recommended Model |\n",
    "|----------|------------------|\n",
    "| Simple reporting, stakeholder communication | Additive |\n",
    "| Strong channel interactions expected | Multiplicative (Lift) |\n",
    "| Quick elasticity estimates needed | Log-Log |\n",
    "| Using tools like Robyn, PyMC-Marketing | Additive (default) |\n",
    "| Using Google Meridian | Multiplicative |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTIPLICATIVE MMM - EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n--- LOG-LOG MODEL ---\")\n",
    "print(f\"Model Fit: R2 = {r2_loglog:.3f}, MAPE = {mape_loglog:.1f}%\")\n",
    "print(\"\\nElasticities (1% spend increase -> X% sales increase):\")\n",
    "for i, ch in enumerate(media_channels):\n",
    "    print(f\"  {ch.replace('_SPEND', '')}: {elasticities[i]:.4f}\")\n",
    "\n",
    "print(\"\\n--- LIFT-FACTOR MODEL ---\")\n",
    "print(f\"Model Fit: R2 = {r2_lift:.3f}, MAPE = {mape_lift:.1f}%\")\n",
    "print(f\"Baseline Sales: {baseline_mean:,.0f}/week\")\n",
    "print(\"\\nMaximum Lift Potential:\")\n",
    "for i, ch in enumerate(media_channels):\n",
    "    print(f\"  {ch.replace('_SPEND', '')}: +{gamma_means[i]*100:.1f}%\")\n",
    "\n",
    "print(\"\\n--- SHAPLEY ROI ---\")\n",
    "for ch in media_channels:\n",
    "    print(f\"  {ch.replace('_SPEND', '')}: {roi_shapley[ch]:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "1. **Compare with Additive Model**: Run the main notebook and compare ROI estimates\n",
    "2. **Validate with Experiments**: Test elasticity/lift estimates against geo-lift results\n",
    "3. **Sensitivity Analysis**: Test how results change with different priors\n",
    "4. **Time-Varying Effects**: Consider if elasticities change over time\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook implements concepts from Chapter 9b and Appendix G of the MMM Complete Guide.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}